{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq llama_index==\"0.8.22\" pydantic nltk sentence_transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleWebPageReader\n",
    "\n",
    "urls = [\"http://paulgraham.com/greatwork.html\"]\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(urls)\n",
    "assert len(documents) == 1\n",
    "\n",
    "# TODO: Replace Note, ex) [1]\n",
    "# documents[0].text.find(\"[1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, LLMPredictor\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "\n",
    "def generate_questions(model, num=None):\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=model))\n",
    "    node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=128)\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor, node_parser=node_parser\n",
    "    )\n",
    "\n",
    "    data_generator = DatasetGenerator.from_documents(\n",
    "        documents,\n",
    "        service_context,\n",
    "        num_questions_per_chunk=5,\n",
    "    )\n",
    "    questions = data_generator.generate_questions_from_nodes(num)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "def cluster(questions):\n",
    "    embeddings = embedder.encode(questions)\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=2)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "\n",
    "    clustered_sentences = {}\n",
    "\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        if cluster_id not in clustered_sentences:\n",
    "            clustered_sentences[cluster_id] = []\n",
    "        clustered_sentences[cluster_id].append(questions[sentence_id])\n",
    "\n",
    "    return clustered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = generate_questions(model=\"gpt-3.5-turbo\")\n",
    "c = cluster(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 2\n",
      "55 3\n",
      "19 3\n",
      "16 2\n",
      "1 3\n",
      "3 5\n",
      "69 1\n",
      "9 3\n",
      "61 2\n",
      "12 4\n",
      "8 4\n",
      "5 2\n",
      "20 2\n",
      "56 1\n",
      "41 2\n",
      "86 1\n",
      "40 2\n",
      "80 1\n",
      "2 3\n",
      "67 1\n",
      "22 3\n",
      "54 2\n",
      "4 3\n",
      "26 2\n",
      "35 2\n",
      "28 2\n",
      "14 2\n",
      "11 3\n",
      "68 1\n",
      "87 1\n",
      "45 2\n",
      "7 2\n",
      "77 1\n",
      "42 2\n",
      "65 1\n",
      "70 1\n",
      "51 1\n",
      "39 3\n",
      "63 1\n",
      "52 2\n",
      "76 2\n",
      "23 2\n",
      "21 2\n",
      "71 1\n",
      "57 1\n",
      "18 3\n",
      "30 2\n",
      "29 2\n",
      "62 2\n",
      "0 3\n",
      "66 1\n",
      "72 1\n",
      "47 2\n",
      "75 1\n",
      "53 2\n",
      "25 2\n",
      "43 1\n",
      "10 3\n",
      "17 2\n",
      "85 1\n",
      "73 1\n",
      "44 1\n",
      "33 1\n",
      "27 2\n",
      "81 1\n",
      "84 2\n",
      "64 1\n",
      "34 1\n",
      "15 3\n",
      "60 1\n",
      "13 4\n",
      "48 1\n",
      "82 1\n",
      "50 2\n",
      "24 4\n",
      "46 1\n",
      "32 1\n",
      "79 1\n",
      "49 1\n",
      "78 1\n",
      "83 1\n",
      "59 1\n",
      "74 1\n",
      "38 1\n",
      "58 1\n",
      "36 1\n",
      "6 2\n",
      "31 1\n"
     ]
    }
   ],
   "source": [
    "for k in c.keys():\n",
    "    print(k, len(c[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    balanced: Dataset({\n",
       "        features: ['question'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    raw: Dataset({\n",
       "        features: ['question'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "raw = Dataset.from_dict({\"question\": []})\n",
    "balanced = Dataset.from_dict({\"question\": []})\n",
    "\n",
    "d = DatasetDict({\"balanced\": balanced, \"raw\": raw})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a8c4a609994a26b6dfe02df948c262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef8fd4e2a854d28b7e64700207b58b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format: 0ba [00:00, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e43e5a79d644b4b5c461c935730168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a43ab27fbb444e9a881d25190111e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format: 0ba [00:00, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d.push_to_hub(\"fastrepl/questions_pg_how_to_do_great_work\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
